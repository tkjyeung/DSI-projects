{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Lab\n",
    "\n",
    "In this lab we will explore feature selection on the Titanic Dataset. First of all let's load a few things:\n",
    "\n",
    "- Standard packages\n",
    "- The training set from lab 2.3\n",
    "- The union we have saved in lab 2.3\n",
    "\n",
    "\n",
    "You can load the titanic data as follows:\n",
    "\n",
    "    psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student titanic\n",
    "    password: gastudents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://dsi_student:gastudents@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com/titanic')\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM train', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5924806 ,  0.        ,  0.        , ...,  1.        ,\n",
       "         1.        , -0.50244517],\n",
       "       [ 0.63878901,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.78684529],\n",
       "       [-0.2846632 ,  0.        ,  0.        , ...,  1.        ,\n",
       "         0.        , -0.48885426],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  1.        ,\n",
       "         0.        , -0.17626324],\n",
       "       [-0.2846632 ,  1.        ,  0.        , ...,  0.        ,\n",
       "         1.        , -0.04438104],\n",
       "       [ 0.17706291,  0.        ,  0.        , ...,  0.        ,\n",
       "         1.        , -0.49237783]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import dill\n",
    "\n",
    "with gzip.open('../../../2.3-lab/assets/datasets/union.dill.gz') as fin:\n",
    "    union = dill.load(fin)\n",
    "    \n",
    "X = df[[u'Pclass', u'Sex', u'Age', u'SibSp', u'Parch', u'Fare', u'Embarked']]\n",
    "y = df[u'Survived']\n",
    "\n",
    "X_transf = union.fit_transform(X)\n",
    "X_transf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1 Column names\n",
    "\n",
    "Uh oh, we have lost the column names along the way! We need to manually add them:\n",
    "- age_pipe => 'scaled_age'\n",
    "- one_hot_pipe => 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S'\n",
    "- gender_pipe => 'male'\n",
    "- fare_pipe => 'scaled_fare'\n",
    "\n",
    "Now we need to:\n",
    "\n",
    "1. Create a new pandas dataframe called `Xt` with the appropriate column names and fill it with the `X_transf` data.\n",
    "2. Notice that the current pipeline complitely discards the columns: u'SibSp', u'Parch'. Stack them as they are to the new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xt = pd.DataFrame(data=X_transf, columns=['scaled_age', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S',  'male', 'scaled_fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection\n",
    "\n",
    "Let's use the `SelectKBest` method in scikit learn to see which are the top 5 features.\n",
    "\n",
    "- What are the top 5 features for `Xt`?\n",
    "\n",
    "=> store them in a variable called `kbest_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = SelectKBest(k=5).fit_transform(Xt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(data=X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selectk = SelectKBest(k=5).fit(Xt, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SelectFromModel(selectk, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The underlying estimator SelectKBest has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-a752635296e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/vagrant/.anaconda/lib/python2.7/site-packages/sklearn/feature_selection/base.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \"\"\"\n\u001b[0;32m     75\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             warn(\"No features were selected: either the data is\"\n",
      "\u001b[1;32m/home/vagrant/.anaconda/lib/python2.7/site-packages/sklearn/feature_selection/base.pyc\u001b[0m in \u001b[0;36mget_support\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mindices\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \"\"\"\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_support_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vagrant/.anaconda/lib/python2.7/site-packages/sklearn/feature_selection/from_model.pyc\u001b[0m in \u001b[0;36m_get_support_mask\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[1;34m'Either fit the model before transform or set \"prefit=True\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                 ' while passing the fitted estimator to the constructor.')\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_feature_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         self.threshold_ = _calculate_threshold(estimator, scores,\n\u001b[0;32m    203\u001b[0m                                                self.threshold)\n",
      "\u001b[1;32m/home/vagrant/.anaconda/lib/python2.7/site-packages/sklearn/feature_selection/from_model.pyc\u001b[0m in \u001b[0;36m_get_feature_importances\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;34m\"`feature_importances_` attribute. Either pass a fitted estimator\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;34m\" to SelectFromModel or call fit before calling transform.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             % estimator.__class__.__name__)\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The underlying estimator SelectKBest has no `coef_` or `feature_importances_` attribute. Either pass a fitted estimator to SelectFromModel or call fit before calling transform."
     ]
    }
   ],
   "source": [
    "X_new = model.transform(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'SelectKBest' has no attribute 'coef__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8e2756d6ece5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'SelectKBest' has no attribute 'coef__'"
     ]
    }
   ],
   "source": [
    "SelectKBest.coef__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recursive Feature Elimination\n",
    "\n",
    "`Scikit Learn` also offers recursive feature elimination as a class named `RFECV`. Use it in combination with a logistic regression model to see what features would be kept with this method.\n",
    "\n",
    "=> store them in a variable called `rfecv_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic regression coefficients\n",
    "\n",
    "Let's see if the Logistic Regression coefficients correspond.\n",
    "\n",
    "- Create a logistic regression model\n",
    "- Perform grid search over penalty type and C strength in order to find the best parameters\n",
    "- Sort the logistic regression coefficients by absolute value. Do the top 5 correspond to those above?\n",
    "> Answer: Not completely. That could be due to scaling\n",
    "\n",
    "=> choose which ones you would keep and store them in a variable called `lr_columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare features sets\n",
    "\n",
    "Use the `best estimator` from question 4 on the 3 different feature sets:\n",
    "\n",
    "- `kbest_columns`\n",
    "- `rfecv_columns`\n",
    "- `lr_columns`\n",
    "- `all_columns`\n",
    "\n",
    "Questions:\n",
    "\n",
    "- Which scores the highest? (use cross_val_score)\n",
    "- Is the difference significant?\n",
    "> Answer: Not really\n",
    "- discuss in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Use a bar chart to display the logistic regression coefficients. Start from the most negative on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
