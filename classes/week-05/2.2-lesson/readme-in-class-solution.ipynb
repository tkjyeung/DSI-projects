{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pipelines and custom transfomers in SKLearn\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Pipelines and custom transfomers in SKLearn\n",
    "Week 5 | Lesson 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- create pipelines for cleaning and manipulating data\n",
    "- use pipelines to preprocess data from the SQL database\n",
    "- use pipeline in combination with classification\n",
    "- create a custom transformer using the `TransformerMixin` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- extract data from a database\n",
    "- perform classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTOR PREP\n",
    "*Before this lesson, instructors will need to:*\n",
    "- Read in / Review any dataset(s) & starter/solution code\n",
    "- Generate a brief slide deck\n",
    "- Prepare any specific materials\n",
    "- Provide students with additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LESSON GUIDE\n",
    "\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 mins | [Opening](#opening) | Opening |\n",
    "| 10 mins | [Introduction](#introduction) | Data Pipelines |\n",
    "| 25 mins | [Demo](#demo) | Pipelines in scikit-learn |\n",
    "| 15 mins | [Guided-practice](#guided-practice) | Make Pipeline and the preprocessing module |\n",
    "| 10 minutes | [Demo](#demo_2) | Custom Transformers |\n",
    "| 20 minutes | [Ind-practice](#ind-practice) | Putting it all together |\n",
    "| 5 mins | [Conclusion](#conclusion) | Conclusion |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 mins)\n",
    "Many organizations rely on data engineering teams to encode these common tasks into pipelines. **Data pipelines** are a series of automated data transformations that ensure the validity of your work for routine data maintenance tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Data Pipelines (10 mins)\n",
    "\n",
    "The term _Pipeline_ is used to indicate a series of concatenated data transformations. Each stage of the pipeline feeds from the previous stage, i.e. the output of a stage is plugged into the input of the next stage and data flows through the pipeline from beginning to end as water flows through... a pipeline.\n",
    "\n",
    "![Pipeline](./assets/images/pipeline.png)\n",
    "\n",
    "Each processing stage has an input, where data comes in, and an output, where processed data comes out.\n",
    "\n",
    "**Check:** Ask the students to give some examples of data transformations.\n",
    "\n",
    "Pipelines provide a higher level of abstraction than the individual building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo\"></a>\n",
    "## Pipelines in scikit-learn (25 mins)\n",
    "One way to improve coding and model management is to use pipelines in `scikit-learn`. These tie together all the steps that you may need to prepare your dataset and make your predictions. Because you will need to perform all of the exact same transformations on your evaluation data, encoding the exact steps is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how a pipeline works, we'll use an example involving Natural Language Processing. Data comes from the [Evergreen Stumbleupon Kaggle Competition](https://www.kaggle.com/c/stumbleupon/data), where participants where challenged to build a classifier to categorize webpages as evergreen or non-evergreen. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) were provided. We'll focus on the page title text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    IBM Sees Holographic Calls Air Breathing Batte...\n",
       "1    The Fully Electronic Futuristic Starting Gun T...\n",
       "2    Fruits that Fight the Flu fruits that fight th...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"./datasets/stumbleupon.tsv\", sep='\\t')\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "\n",
    "titles = data['title'].fillna('')\n",
    "titles[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['label']\n",
    "y[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51332\n",
       "0    0.48668\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each datapoint is a string of free form text. How can we feed this to a model? The simplest way is to build a dictionary of words and use those as features. This is what a `CountVectorizer` does.\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "|Sentence|the|cat|is|on|table|blue|\n",
    "|---|---|---|---|---|---|---|\n",
    "|The cat is on the table|2|1|1|1|1|0|\n",
    "|The table is blue|1|0|1|0|1|1|\n",
    "|...||||||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=True, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.9, max_features=1000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000,\n",
    "                             ngram_range=(1, 2),\n",
    "                             stop_words='english', \n",
    "                             min_df=1,\n",
    "                             max_df=0.9,\n",
    "                             binary=True)\n",
    "\n",
    "vectorizer.fit(['IBM Sees the Holographic Calls Air Breathing Holographic', 'This is the second document hot air'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'air breathing',\n",
       " u'breathing',\n",
       " u'breathing holographic',\n",
       " u'calls',\n",
       " u'calls air',\n",
       " u'document',\n",
       " u'document hot',\n",
       " u'holographic',\n",
       " u'holographic calls',\n",
       " u'hot',\n",
       " u'hot air',\n",
       " u'ibm',\n",
       " u'ibm sees',\n",
       " u'second',\n",
       " u'second document',\n",
       " u'sees',\n",
       " u'sees holographic']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['IBM Sees Holographic Air']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air breathing</th>\n",
       "      <th>breathing</th>\n",
       "      <th>breathing holographic</th>\n",
       "      <th>calls</th>\n",
       "      <th>calls air</th>\n",
       "      <th>document</th>\n",
       "      <th>document hot</th>\n",
       "      <th>holographic</th>\n",
       "      <th>holographic calls</th>\n",
       "      <th>hot</th>\n",
       "      <th>hot air</th>\n",
       "      <th>ibm</th>\n",
       "      <th>ibm sees</th>\n",
       "      <th>second</th>\n",
       "      <th>second document</th>\n",
       "      <th>sees</th>\n",
       "      <th>sees holographic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   air breathing  breathing  breathing holographic  calls  calls air  \\\n",
       "0              0          0                      0      0          0   \n",
       "1              0          0                      0      0          0   \n",
       "2              0          0                      0      0          0   \n",
       "\n",
       "   document  document hot  holographic  holographic calls  hot  hot air  ibm  \\\n",
       "0         0             0            1                  0    0        0    1   \n",
       "1         0             0            0                  0    0        0    0   \n",
       "2         0             0            0                  0    1        1    1   \n",
       "\n",
       "   ibm sees  second  second document  sees  sees holographic  \n",
       "0         1       0                0     1                 1  \n",
       "1         0       0                0     0                 0  \n",
       "2         0       0                0     0                 0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectorizer.transform(['IBM Sees Holographic Air',\n",
    "                                   'This is a test', \n",
    "                                   \"IBM is just pure hot air air\"]).todense(), \n",
    "             columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check**: What is the meaning of the various parameters used at initialization of the Vectorizer?\n",
    "\n",
    "\n",
    "Let's use the vectorizer to fit all the titles and build a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)\n",
    "\n",
    "# X = vectorizer.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used this input X, a matrix of all common n-grams in the dataset, as an input to our classifier. We wanted to classify how evergreen a story was, based on these inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV scores: [ 0.74574209  0.75659229  0.75487013]\n",
      "Average CVScore: 0.752 +/- 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, cross_validation\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "scores = cross_validation.cross_val_score(model, X, y)\n",
    "\n",
    "print('CV scores: {}'.format(scores))\n",
    "\n",
    "print('Average CVScore: {:0.3f} +/- {:0.3f}'.format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Steps in Pipelines\n",
    "\n",
    "Often we will want to combine these steps to evaluate some future dataset. Therefore, we need to make sure we perform the _exact same_ transformations on the data. If \"has_brownies_in_text\" is column 19, we need to make sure it is __also__ column 19 during future evaluation.\n",
    "\n",
    "Pipelines combines both pre-processing and model building steps into a _single object_. Rather than manually evaluating the transformers and then feeding them into the models, pipelines ties both of these steps together.\n",
    "\n",
    "Similar to models and vectorizers in scikit-learn, pipelines are equipped with `fit` and `predict` or `predict_proba` methods (as any model would be), but they ensure that proper data transformations are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46801259,  0.53198741],\n",
       "       [ 0.28316885,  0.71683115],\n",
       "       [ 0.00513415,  0.99486585],\n",
       "       ..., \n",
       "       [ 0.2906378 ,  0.7093622 ],\n",
       "       [ 0.60684131,  0.39315869],\n",
       "       [ 0.66320386,  0.33679614]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into a training set\n",
    "training_data = data[:6000]\n",
    "X_train = training_data['title'].fillna('')\n",
    "y_train = training_data['label']\n",
    "\n",
    "# These rows are rows obtained in the future, unavailable at training time\n",
    "X_new = data[6000:]['title'].fillna('')\n",
    "\n",
    "pipe = pipeline.Pipeline([\n",
    "        ('vec', vectorizer),\n",
    "        ('model', model)   \n",
    "    ])\n",
    "\n",
    "# Fit the full pipeline\n",
    "# This means we perform the steps laid out above\n",
    "# First we fit the vectorizer,\n",
    "# And then feed the output of that into the fit function of the model\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Here again we apply the full pipeline for predictions\n",
    "# The text is transformed automatically to match the features from the pipeline\n",
    "pipe.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Add a `MaxAbsScaler` scaling step to the pipeline, which should occur after the vectorization.\n",
    "\n",
    "#### Merging Feature Sets in Pipelines\n",
    "\n",
    "Additionally, we want to merge many different feature sets automatically. Here we can use scikit-learn's `FeatureUnion`.\n",
    "\n",
    "While scikit-learn pipelines help with managing the transformation from raw data, there may be many steps before this takes place in your pipeline. These pipelines are often referred to as _ETL pipelines_ for \"Extract, Transform, Load.\"\"\n",
    "\n",
    "In an _ETL pipeline_, the data is pulled or extracted from some source (like a database), transformed or manipulated, and then \"loaded\" into whatever system or analysis requires them.\n",
    "\n",
    "Many data science teams rely on software tools to manage these ETL pipelines. If a transformation step fails, these tools alert you, or ensure that step can be re-run. If these transformation steps need to happen daily or weekly, these tools can manage that timeline.\n",
    "\n",
    "- One of the most popular Python tools for this is [Luigi](https://github.com/spotify/luigi) developed by Spotify.\n",
    "- An alternative is [Airflow](https://github.com/airbnb/airflow) by AirBnB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Make Pipeline and the preprocessing module (15 mins)\n",
    "\n",
    "#### make_pipeline\n",
    "Scikit-learn pipelines can also be built using the `make_pipeline` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe1 = pipeline.make_pipeline(StandardScaler(), LogisticRegression())    \n",
    "\n",
    "pipe2 = pipeline.Pipeline(steps=[('standardscaler',StandardScaler()),\n",
    "                                 ('logisticregression',LogisticRegression())\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two pipelines created above are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing in sklearn (in pairs)\n",
    "\n",
    "The preprocessing module comes loaded with many very useful pre-processing classes.\n",
    "\n",
    "**Check** in pairs, assign one function to each pair, they have to read about it in the doc and then explain it to the class.\n",
    "\n",
    "\n",
    "Data Manipulators\n",
    "- Binarizer\n",
    "- KernelCenterer\n",
    "- MaxAbsScaler\n",
    "- MinMaxScaler\n",
    "- Normalizer\n",
    "- OneHotEncoder\n",
    "- PolynomialFeatures\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "Data Imputation\n",
    "- Imputer\n",
    "\n",
    "Function Transformer\n",
    "- FunctionTransformer\n",
    "\n",
    "Label Manipulators\n",
    "- LabelBinarizer\n",
    "- LabelEncoder\n",
    "- MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo_2\"></a>\n",
    "## Custom Transformers (10 minutes)\n",
    "\n",
    "We can implement custom transformers by extending the BaseClass in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 2 0 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 4]]\n",
      "[[  2.5   0.    0.    0. ]\n",
      " [  0.    5.    0.    0. ]\n",
      " [  0.    0.    7.5   0. ]\n",
      " [  0.    0.    0.   10. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FeatureMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X * self.factor\n",
    "    \n",
    "\n",
    "fm = FeatureMultiplier(2.5)\n",
    "\n",
    "test = np.diag((1,2,3,4))\n",
    "print test\n",
    "\n",
    "# print fm.transform(test)\n",
    "print fm.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Compare this with the `FunctionTransformer` from the preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.5,   0. ,   0. ,   0. ],\n",
       "       [  0. ,   5. ,   0. ,   0. ],\n",
       "       [  0. ,   0. ,   7.5,   0. ],\n",
       "       [  0. ,   0. ,   0. ,  10. ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "ft = preprocessing.FunctionTransformer(lambda x: x*2.5)\n",
    "\n",
    "ft.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check** Implement a custom transformer that selects a specific feature from a Pandas dataframe. It should be initialized with the column name or the column index and it should return the selected column when transforming a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'url', u'urlid', u'boilerplate', u'alchemy_category',\n",
      "       u'alchemy_category_score', u'avglinksize', u'commonlinkratio_1',\n",
      "       u'commonlinkratio_2', u'commonlinkratio_3', u'commonlinkratio_4',\n",
      "       u'compression_ratio', u'embed_ratio', u'framebased', u'frameTagRatio',\n",
      "       u'hasDomainLink', u'html_ratio', u'image_ratio', u'is_news',\n",
      "       u'lengthyLinkDomain', u'linkwordscore', u'news_front_page',\n",
      "       u'non_markup_alphanum_characters', u'numberOfLinks', u'numwords_in_url',\n",
      "       u'parametrizedLinkRatio', u'spelling_errors_ratio', u'label', u'title',\n",
      "       u'body'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>linkwordscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   linkwordscore\n",
       "0             24\n",
       "1             40\n",
       "2             55\n",
       "3             24\n",
       "4             14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class SelectTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        #return X.ix[:, self.column]\n",
    "        return X[self.columns]\n",
    "    \n",
    "    # def fit_transform(self, X, y=None):\n",
    "    #     print \"Jacques stop doing that! Don't overwrite me!!!\"                \n",
    "    #     return self.fit(X, y).transform(X, y)    \n",
    "    \n",
    "st = SelectTransformer(['linkwordscore'])\n",
    "st.fit_transform(data).head()\n",
    "\n",
    "# ft = preprocessing.FunctionTransformer(lambda x: x[['urlid','numberOfLinks']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Union example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Title: (7395,)\n",
      "After TextImputter: (7395,)\n",
      "After CountVectorizer: (7395, 10557)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7395x10557 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 51034 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelectOneColumn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # return X.ix[:, self.column]\n",
    "        return X[self.column]\n",
    "    \n",
    "class TextImputter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, input_with = ''):\n",
    "        self.input_with = input_with\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        def replace_with(x):\n",
    "            return x if x is not None else self.input_with            \n",
    "\n",
    "        return X.apply(replace_with)\n",
    "\n",
    "class PrintShape(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, pre_str = ''):\n",
    "        self.pre_str = pre_str\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print \"{}: {}\".format(self.pre_str, X.shape)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "pipeTxt = pipeline.make_pipeline(SelectOneColumn('title'), PrintShape('After Title'),\n",
    "                                 TextImputter(), PrintShape('After TextImputter'),\n",
    "                                 CountVectorizer(), PrintShape('After CountVectorizer'))\n",
    "pipeTxt.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Title: (7395,)\n",
      "After TextImputter: (7395,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pizzato/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After CountVectorizer: (7395, 10557)\n",
      "Shape of after Union: (7395, 10559)\n"
     ]
    }
   ],
   "source": [
    "pipeNum1 = pipeline.make_pipeline(SelectTransformer(['numberOfLinks']), preprocessing.MinMaxScaler())\n",
    "pipeNum2 = pipeline.make_pipeline(SelectTransformer(['linkwordscore']), preprocessing.Normalizer())\n",
    "\n",
    "featunion = pipeline.make_union(pipeNum1, pipeNum2, pipeTxt)\n",
    "new_data = featunion.fit_transform(data)\n",
    "print \"Shape of after Union:\", new_data.shape\n",
    "\n",
    "\n",
    "#pipeline.make_pipeline(featunion, linear_model.LogisticRegression())\n",
    "#pipeTxt.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ind-practice\"></a>\n",
    "## Putting it all together (20 minutes)\n",
    "\n",
    "**Check** Revisit the dataset of lab 1.4. How could you use `make_pipeline` and `make_union` to build a pipeline that performs the same steps all in one pass?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 mins)\n",
    "We have learnt how to use the `Pipeline` construct in order to chain several instructions in one single class. This enables to treat data-processing from a more abstract and more powerful perspective, and it's a pre-cursor to the work we will do when working with Big Data technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Pipelines and Feature Union](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "- [Example with complex pipeline](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#example-hetero-feature-union-py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
