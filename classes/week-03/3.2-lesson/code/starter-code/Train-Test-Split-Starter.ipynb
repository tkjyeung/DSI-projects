{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We've discussed overfitting in the context of bias and variance, and we've seen some techniques like regularization that are used to avoid overfitting. In this lesson we'll discuss another method for avoid overfitting that is commonly referred to a the _train/test split_. The idea is very similar to cross-validation (indeed it is a type of cross-validation) in that we split the dataset into two subsets:\n",
    "* a subset to train our model on, and\n",
    "* a subset to test our model's predictions on\n",
    "\n",
    "This serves two useful purposes:\n",
    "* We prevent overfitting by not using all the data, and\n",
    "* we have some remaining data to evaluate our model.\n",
    "\n",
    "While it may seem like a relatively simple idea, there are some caveats to putting it into practice. For example, if you are not careful it is easy to take a non-random split. Suppose we have salary data on technical professionals that is composed 80% of data from California and 20% elsewhere and is sorted by state. If we split our data into 80% training data and 20% testing data we ight inadvertantly select all the California data to train and all the non-California data to test. In this case we've still overfit on our data set because we did not sufficiently randomize the data.\n",
    "\n",
    "In a situation like this we can use _k-fold cross validation_, which is the same idea applied to more than two subsets. In particular, we partition our data into $k$ subsets and train on $k-1$ one of them. holding the last slice for testing. We can do this for each of the possible $k-1$ subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo\n",
    "Let's explore test-training split with some sample datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Make the plots bigger\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split()\n",
    "diabetes = datasets.load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data, columns=columns)\n",
    "y = diabetes.target\n",
    "# Take a look at the data again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a nice function to split a dataset for testing and training called `train_test_split`. The `test_size` keyword argument indicates the proportion of the data that should be held over for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.4)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit a model on the training data and test on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)\n",
    "\n",
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "\n",
    "print \"Score:\", model.score(X_test, y_test)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could always split the data up manually. Here's an example for [this dataset](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) of a manual splitting.\n",
    "\n",
    "Now let's try out k-fold cross-validation. Again scikit-learn provides useful functions to do the heavy lifting. The function `cross_val_predict` returns the predicted values for each data point when it's in the testing slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "cv = KFold(len(df), 3)\n",
    "\n",
    "\n",
    "# Perform 3-fold cross validation\n",
    "scores = cross_val_score(model, df, y, cv=cv)\n",
    "print \"Cross-validated scores:\", scores\n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, df, y, cv=cv)\n",
    "colors = ['red','blue','green','cyan']\n",
    "for i, (fold_train, fold_test) in enumerate(cv):\n",
    "    plt.scatter(y[fold_test], predictions[fold_test], color=colors[i])\n",
    "accuracy = metrics.r2_score(y, predictions)\n",
    "print \"Cross-Predicted Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, df, y, cv=6)\n",
    "print \"Cross-validated scores:\", scores\n",
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, df, y, cv=6)\n",
    "plt.scatter(y, predictions)\n",
    "accuracy = metrics.r2_score(y, predictions)\n",
    "print \"Cross-Predicted Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Practice\n",
    "\n",
    "Use what you've learned to train and test models on the Boston housing data set. If you need a few hints take a look at [this example](http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html) but try your best to make it happen first. Complete the following tasks:\n",
    "* Fit a linear model to the Boston Housing data using all the available variables. Perform test training splits of 50:50, 70:30, and 90:10, comparing the scores on test data.\n",
    "* For the same setup, perform a $k$-fold cross validation with $k=5$ slices (with cross-validated predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr = linear_model.LinearRegression()\n",
    "boston = datasets.load_boston()\n",
    "df = pd.DataFrame(boston.data)\n",
    "y = boston.target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Practice\n",
    "\n",
    "Ultimately we use a test-training split to compare multiple models on the same dataset. This could be comparisons of two linear models, or of completely different models on the same data.\n",
    "\n",
    "For your independent practice, fit three different models on the Boston housing data. For example, you could pick three different subsets of variables, one or more polynomial models, or any other model that you like. Then:\n",
    "* Fix a testing/training split of the data\n",
    "* Train each of your models on the training data\n",
    "* Evaluate each of the models on the test data\n",
    "* Rank the models by how well they score on the testing data set.\n",
    "\n",
    "Bonus tasks:\n",
    "* Try a few different splits of the data for the same models. Does your ranking change?\n",
    "* Perform a k-fold cross validation and use the cross-validation scores to compare your models. Did this change your rankings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
