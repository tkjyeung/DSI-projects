{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the data we used in the previous codealong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "column_names = ['id',\n",
    "                'clump_thickness',\n",
    "                'cell_size_uniformity',\n",
    "                'cell_shape_uniformity',\n",
    "                'marginal_adhesion',\n",
    "                'single_epithelial_size',\n",
    "                'bare_nuclei',\n",
    "                'bland_chromatin',\n",
    "                'normal_nucleoli',\n",
    "                'mitoses',\n",
    "                'class']\n",
    "\n",
    "bcw = pd.read_csv('../assets/datasets/breast-cancer-wisconsin.csv',\n",
    "                 names=column_names, na_values=['?'])\n",
    "\n",
    "bcw.dropna(inplace=True)\n",
    "bcw['metrics_pct'] = bcw[[x for x in column_names if x not in ['class','id']]].sum(axis=1)/90.\n",
    "bcw['class'] = bcw['class'].apply(lambda x: 0 if x == 2 else 1)\n",
    "\n",
    "metrics_pct = np.array(bcw.metrics_pct.values)\n",
    "metrics_pct = metrics_pct[:, np.newaxis]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(metrics_pct, bcw[['class']].values, \n",
    "                                                    test_size=0.33, stratify=bcw[['class']].values,\n",
    "                                                    random_state=77)\n",
    "\n",
    "logreg = LogisticRegression(random_state=77)\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use logreg.predict, what is actually happening here?\n",
    "\n",
    "The logreg model determines a **predicted probability** of class 0 and class 1 for each observation in the X_test matrix. These probabilities sum to 1 across classes. When predict is called, the standard behavior is to assign the class with the greater probability.\n",
    "\n",
    "Recreate the Y_pred vector manually from the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the predicted probability vector\n",
    "Y_pp = pd.DataFrame(logreg.predict_proba(X_test), columns=['class_0_pp','class_1_pp'])\n",
    "print(Y_pp.iloc[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavior is equivalent to a **classification threshold of 0.5 for class 1**. In other words, if class 1's predicted probability is >= 0.5, the observation is predicted to be class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pp['pred_class_thresh50'] = Y_pred\n",
    "print(Y_pp.iloc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say again that we are predicting cancer based on some kind of detection measure, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conmat = np.array(confusion_matrix(Y_test, Y_pred, labels=[1,0]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['has_cancer', 'is_healthy'],\n",
    "                         columns=['predicted_cancer','predicted_healthy'])\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, we may be particularly interested in minimizing the false positive rate, aka reducing the chance that a patient with cancer is diagnosed as healthy.\n",
    "\n",
    "In order to do this, we can **lower the threshold for predicting class 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_pp['pred_class_thresh10'] = [1 if x >= 0.10 else 0 for x in Y_pp.class_1_pp.values]\n",
    "print(Y_pp.iloc[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will reduce our false negative rate to 0, but at the expense of our false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conmat_10 = np.array(confusion_matrix(Y_test, Y_pp.pred_class_thresh10.values, labels=[1,0]))\n",
    "\n",
    "confusion_10 = pd.DataFrame(conmat_10, index=['has_cancer', 'is_healthy'],\n",
    "                            columns=['predicted_cancer','predicted_healthy'])\n",
    "\n",
    "print(confusion_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way to visualize these tradeoffs is with **area under the curve (AUC)** graphs. The most popular AUC graph is the **reciever operating characteristic (ROC)** curve.\n",
    "\n",
    "\n",
    "- True positive rate vs false negative rate\n",
    "- Only compares correct vs incorrect\n",
    "- Easily plotted using roc_curve() and auc() from sklearn\n",
    "\n",
    "The ROC curve compares the true positive rate against the false positive rate. It is unaffected by the distribution of class labels since it is only comparing the correct vs. incorrect label assignments for one class.\n",
    "\n",
    "To plot this we will use the roc_curve() and auc() functions from sklearn. We will also use the decision_function() method of the logistic regression, which essentiall gives us the confidence of each observation being in one class or another.\n",
    "\n",
    "(Plotting is the nearly the same as in the scikit learn documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "Y_score = logreg.decision_function(X_test)\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(Y_test, Y_score)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for cancer detection', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the intuition for the ROC curve?\n",
    "\n",
    "As the class assignment threshold increases for the positive class (has cancer), the false positive rate and true positive rate necessarily increase. For a classifier performing at chance, this would be the diagonal dotted line: an equal chance of false positives and true positives. \n",
    "\n",
    "- The greater the area under the curve, the better higher the quality of the model\n",
    "\n",
    "The greater the area under the curve, the higher the ratio of true positives to false positives as the threshold becomes more lenient. Thus, the greater the area under the curve, the higher the quality of the classification model. In the Wisconsin breast cancer data the area under the curve is 0.99, indicating a nearly perfect model. Most classification problems will never get close to this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
